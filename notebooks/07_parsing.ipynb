{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Dependency Parsing with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "%matplotlib notebook\n",
    "import pandas as pd\n",
    "df = pd.read_csv('death-penalty-cases.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Science cannot solve the ultimate mystery of nature. And that is because, in the last analysis, we ourselves are a part of the mystery that we are trying to solve.'\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in doc.sents:\n",
    "    print(\"sentence:\", sent)\n",
    "    print(\"root:\", sent.root)\n",
    "    print([(w, w.dep_) for w in sent.root.children])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_display = nlp('Science cannot solve the ultimate mystery of nature.')\n",
    "to_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(to_display, style=\"dep\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noun Phrase Chunking\n",
    "list(doc.noun_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent.root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(sent.root.children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left children\n",
    "list(sent.root.lefts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Right children\n",
    "list(sent.root.rights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first token\n",
    "sent[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first token dependency label, cc=conjunction\n",
    "sent[0].dep_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent[0].head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Discovery of Gendered Language through Latent-Variable Modeling\n",
    "\n",
    "[Hoyle et al. (2019)](https://www.aclweb.org/anthology/P19-1167/) study the language use of gendered nouns and proceed to train a generative latent-variable model that jointly represents adjective (or verb) choice, with its sentiment given the (natural) gender of a noun. To this extent, they extract noun–adjectives pairs, NSUBJ–verb pairs and DOBJ–verb pairs. \n",
    "\n",
    "In the following, we show how to extract NSUBJ-verb pairs from text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(n=2000)\n",
    "df[\"processed\"] = df[\"snippet\"].apply(lambda x: nlp(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subject_verb_pairs(sent):\n",
    "    subjs = [w for w in sent if w.dep_ == \"nsubj\"]\n",
    "    pairs = [(w.lemma_.lower(), w.head.lemma_.lower()) for w in subjs]\n",
    "    return pairs\n",
    "\n",
    "df[\"subj-verb-pairs\"] = df[\"processed\"].apply(lambda x: extract_subject_verb_pairs(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most common pairs\n",
    "from collections import Counter\n",
    "counter = Counter()\n",
    "for item in df[\"subj-verb-pairs\"]:\n",
    "    counter.update(item)\n",
    "    \n",
    "for pair, counts in counter.most_common(n=25):\n",
    "    print (pair, counts) # -pron- is a pronoun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install coreference resolution for spacy\n",
    "!git clone https://github.com/huggingface/neuralcoref.git\n",
    "!cd neuralcoref\n",
    "!pip install -r neuralcoref/requirements.txt\n",
    "!pip install -e neuralcoref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up coreference resolution\n",
    "import neuralcoref      ## ignore RuntimeWarning(s)\n",
    "neuralcoref.add_to_pipe(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coreference Resolution\n",
    "doc = nlp(u'My sister has a dog. She loves him.')\n",
    "print(doc._.has_coref)         ## True\n",
    "print(doc._.coref_clusters)    ## [My sister: [My sister, She], a dog: [a dog, him]]\n",
    "print(doc._.coref_resolved)    ## 'My sister has a dog. My sister loves a dog.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"corefs_resolved\"] = df[\"snippet\"].apply(lambda x: nlp(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subject_verb_pairs_coref(sent):\n",
    "    subjs = [w for w in sent if w.dep_ == \"nsubj\"]\n",
    "    pairs = []\n",
    "    for w in subjs:\n",
    "        # either a subject is part of a coreference chain, then we need to resolve the chain\n",
    "        if w._.in_coref:\n",
    "            cluster = w._.coref_clusters[0]\n",
    "            lemma = cluster.main.root.lemma_.lower()\n",
    "            pairs.append((lemma, w.head.lemma_.lower()))\n",
    "        # if it's not, we can just do the same as above\n",
    "        else:\n",
    "            pairs.append((w.lemma_.lower(), w.head.lemma_.lower()))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"subj-verb-pairs-coref\"] = df[\"corefs_resolved\"].apply(lambda x: extract_subject_verb_pairs_coref(x))\n",
    "counter = Counter()\n",
    "for item in df[\"subj-verb-pairs-coref\"]:\n",
    "    counter.update(item)\n",
    "    \n",
    "for pair, counts in counter.most_common(n=25):\n",
    "    print (pair, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verbs used with defendant\n",
    "\n",
    "for (subject, verb), counts in counter.most_common():\n",
    "    if subject == \"defendant\" and counts > 1:\n",
    "        print (subject, verb, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verbs used with jury\n",
    "\n",
    "for (subject, verb), counts in counter.most_common():\n",
    "    if subject == \"jury\" and counts > 1:\n",
    "        print (subject, verb, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
